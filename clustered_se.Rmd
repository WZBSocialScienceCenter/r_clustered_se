---
title: 'Clustered standard errors with R: Three ways, one result'
author: "Markus Konrad"
date: "4/6/2021"
output:
  pdf_document:
    pandoc_args: ["-Fpandoc-crossref"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

TODO

## Data

We'll work with the dataset *nlswork* that's [included in Stata](https://www.stata-press.com/data/r16/), so we can easily compare the results with Stata. The data comes from the US National Longitudinal Survey (NLS) and contains information about more than 4000 young working women. As for this example, we're interested in the relationship between wage (here as log-scaled GNP-adjusted wage) as dependent variable `ln_wage` and survey participant's current `age`, job `tenure` in years and `union` membership as independent variables. It's a longitudinal survey, so subjects were asked repeatedly between 1968 and 1988 and each subject is identified by an unique `idcode`.

To keep the data size limited, we'll only work with a subset of the data (only subjects with IDs 1 to 100) and we also simply dismiss any observations that contain missing values. **Please note that this is only an example for illustrative purposes on how to apply clustered standard errors in R, not a "real" analysis of the NLS data.**

```{r, warning=FALSE, error=FALSE}
library(webuse)
library(dplyr)

nlswork_orig <- webuse('nlswork')

nlswork <- filter(nlswork_orig, idcode <= 100) %>%
  select(idcode, year, ln_wage, age, tenure, union) %>%
  filter(complete.cases(.)) %>%
  mutate(union = as.integer(union),
         idcode = as.factor(idcode))
str(nlswork)
```
Let's have a look at the first few observations. They contain data from subject #1, who was surveyed several times between 1972 and 1988, and a few observations from subject #2.

```{r}
head(nlswork, 10)
```

```{r}
summary(nlswork)
```

We have 82 subjects in our subset:

```{r}
length(unique(nlswork$idcode))
```
The number of times each subject was surveyed ranges from only once to twelve times:

```{r}
summary(as.integer(table(nlswork$idcode)))
```
In more than one quarter of the observations, the subject answered to be currently member of a trade union:

```{r}
table(nlswork$union)
```

## Fixed-effects model, not adjusting for clustered observations

Our data contains repeated measures for each subject, so we have panel data in which each subject forms a group or cluster. We can use a fixed-effects (FE) model to account for unobserved subject-specific characteristics. We do so by including the subject's `idcode` in our model formula.  It's important to note that `idcode` is of type factor (we applied `idcode = as.factor(idcode)` when we prepared the data) so that for each factor level (i.e. each subject) a FE coefficient will be estimated that represents the subject-specific mean of our dependent variable.

Let's specify and fit such a model using `lm`. We include job `tenure`, `union` membership and an interaction between both. We also control for `age` and add `idcode` as FE variable.

```{r}
m1 <- lm(ln_wage ~ age + tenure + union + tenure:union + idcode,
         data = nlswork)
summary(m1)
```
We're not really interested in the subject-specific means, so let's filter them out and only show our coefficients of interest:

```{r}
m1coeffs_std <- data.frame(summary(m1)$coefficients)
coi_indices <- which(!startsWith(row.names(m1coeffs_std), 'idcode'))
m1coeffs_std[coi_indices,]
```

Unsurprisingly, job tenure and especially union membership are positively associated with wage. (Please note the cautious language here, since we can't make any causal claims with our very simplified approach.) The coefficient of the interaction term shows that union membership also boosts the job tenure effect a bit more, though not significantly.

## Standard errors

In ordinary least squares (OLS) regression, we assume independently and identically distributed (i.i.d) data (among other assumptions). This is clearly *not* the case here: Each subject may be surveyed several times so within each subject's repeated measures, the data are not independent. So these independent observations come from the clustered structured of our data. Although that is not a problem for our regression estimates (they are still unbiased – [Roberts 2013]), it *is* a problem for for the precision of our estimates -- the precision will typically be overestimated, i.e. the standard errors (SEs) will be lower than they should be [Zeileis, Kröll, Graham 2020]. The intuition behind this for our example is that within our clusters we usually have lower variance since the answers come from the same subject and are correlated with each other. This squeezes our estimates' SEs.

(TODO: also point to heteroscedasticity?)

We can deal with this using clustered standard errors with subjects representing our clusters. But before we do this, let's first have a closer look on how OLS estimates' SEs our actually computed.

In matrix notation, a linear model has the form

$$
Y = X\beta + e.
$$
This model has $p$ parameters (including the intercept parameter $\beta_0$) expressed as $p \times 1$ parameter vector $\beta$ and is estimated from $n$ observations in our data. The dependent variable is $Y$ (an $n \times 1$ vector), the independent variables for an $n \times p$ matrix $X$. Finally, the error term $e$ is an $n \times 1$ vector that captures everything that influences $Y$ but cannot be explained by $X\beta$.

By minimizing $e = Y - X\beta$, an estimation for our parameters, $\hat\beta$, can be found. [Roberts 2013] shows how the covariance matrix $V[\hat\beta]$ can be derived which results in the *sandwich estimator*

$$V[\hat\beta] = (X^TX)^{-1} X^T \boldsymbol\Sigma X (X^TX)^{-1}. \;\;\;\;\; (1)$$


This is called sandwich estimator because between two slices of bread $(X^TX)^{-1}$ there is the meat $X^T \boldsymbol\Sigma X$ and this is the most important part for us -- not because we're hungry but because we can see how it relates to the computation of the SEs. One of the classic OLS assumptions is [constant variance (or homoscedasticity)](https://bookdown.org/roback/bookdown-BeyondMLR/ch-MLRreview.html#assumptions-for-linear-least-squares-regression) in the errors across the full spectrum of our dependent variable. This implicates that $\Sigma$ is a diagonal matrix with identical $\hat\sigma^2$ elements. That simplifies the above equation to

$$V[\hat\beta] = \hat\sigma^2 (X^TX)^{-1}. \;\;\;\;\; (2)$$

We're almost finished with calculating the standard errors for a classic OLS model. What's left is the *residual variance* $\hat\sigma^2$. This is calculated as

$$
\hat\sigma^2 = \frac{RSS}{n-p},\\
RSS = \sum^n_{i=1} \hat e_i,
$$
with $\hat e_i$ being the residuals.

Let's replicate the standard errors from model `m1` with our own calculations. To translate these formulae to R, we use `model.matrix()` to get the design matrix $X$, `nobs()` for number of observations $n$, `ncol(X)` for the number or parameters, `solve()` to calculate the inverse of $X^T X$ and `diag()` to extract the diagonal of square matrix.

```{r}
X <- model.matrix(m1)
sigma2 <- sum(residuals(m1)^2) / (nobs(m1) - ncol(X))
crossXinv <- solve(t(X) %*% X, diag(ncol(X)))
m1se <- sqrt(diag(sigma2 * crossXinv))
m1se
```
Let's check if this is equal to the standard errors calculated by `lm()` (using `near()` because of minor deviations due to floating point precision):

```{r}
all(near(m1se, m1coeffs_std$Std..Error))
```
## Clustered robust standard errors

$V[\hat\beta]$ is called the (variance-)covariance matrix and R has the `vcov()` function to calculate it from a fitted model:

```{r}
all(near(sigma2 * crossXinv, vcov(m1)))
```

The square root of the diagonal in the covariance matrix is the SEs of our parameter estimates. Instead of the simplified form for $V[\hat\beta]$ in eq. 2, we can use different estimators for the covariance matrix that are based on the sandwich estimator in eq. 1. The trick with sandwich estimators is that you can exchange the bread and meat of your sandwich according to the structure of your data. This allows you to arrive at consistent SEs even when some of the OLS assumptions like homoscedasticity and independence are violated. This is the "versatile" part in the  Zeileis et al. paper dubbed "Various Versatile Variances".






## Option 1: *sandwich* and *lmtest*

- acts on model from `lm`
- `vcov` either takes a vcov mat or a function to calculate a vcov mat from `lm` object; here, use `vcovCL` function from sandwich
- `cluster` takes the variable(s) to cluster on (either as formula or string vector)

```{r, warning=FALSE}
library(sandwich)
library(lmtest)

m1coeffs_cl <- coeftest(m1, vcov = vcovCL, cluster = ~idcode)     # , fix = TRUE ?
m1coeffs_cl[coi_indices,]
```

- calc. 95%-CIs:

```{r}
coefci(m1, parm = coi_indices, vcov = vcovCL, cluster = ~idcode)
```
- important to "drag along" the vcov and cluster parameters, otherwise CIs for unadjusted model are calculated
- here, `tenure` and `union` CIs suddenly do not include zero

```{r}
coefci(m1, parm = coi_indices)
```

- we can also first calculate the vcov mat for clustered SEs and then pass this matrix to all further functions that we need

```{r}
cl_vcov_mat <- vcovCL(m1, cluster = ~idcode)
heatmap(cl_vcov_mat[1:30, 1:30], Rowv = NA, Colv = NA)
```

- this time we pass the vcov matrix
- result is the same

```{r}
m1coeffs_cl2 <- coeftest(m1, vcov = cl_vcov_mat)
m1coeffs_cl2[coi_indices,]
```
- same for CIs:

```{r}
coefci(m1, parm = coi_indices, vcov = cl_vcov_mat)
```

- calculate marginal effects: pass vcov matrix

```{r, warning=FALSE}
library(margins)

margins(m1, vcov = cl_vcov_mat, variables = 'tenure', at = list(union = 0:1)) %>% summary()
```

- forgetting the vcov matrix:

```{r}
margins(m1, variables = 'tenure', at = list(union = 0:1)) %>% summary()
```

## Option 2: `lm.cluster` from *miceadds*

- once again a strange package name
- additional functionality for imputation with *mice* package
- happens to contain a function for clustered SE, `lm.cluster`
- basically does the same as we've done before by using sandwich's `vcovCL` (see source https://github.com/alexanderrobitzsch/miceadds/blob/ca9e54c18e9743280b9a075e6e119fec38693af2/R/lm.cluster.R#L33 and https://github.com/alexanderrobitzsch/miceadds/blob/ca9e54c18e9743280b9a075e6e119fec38693af2/R/lm_cluster_compute_vcov.R#L13)
- only a bit more convenient maybe

```{r, warning=FALSE}
library(miceadds)

m2 <- lm.cluster(ln_wage ~ age + tenure + union + tenure:union + idcode,
                 cluster = 'idcode',
                 data = nlswork)
m2coeffs <- data.frame(summary(m2))
m2coeffs[!startsWith(row.names(m2coeffs), 'idcode'),]
```

- p-value for intercept is different; dunno why
- returned object from `lm.cluster` is a list that contains the `lm` object as `lmres` and the vcov matrix as `vcov`
- again, these objects need to be "dragged along"
- we also need to pass data again via `data = nlswork`

```{r}
margins(m2$lm_res, vcov = m2$vcov, variables = 'tenure', at = list(union = 0:1), data = nlswork) %>% summary()
```
- the result is consistent with our former computations

## Option 3: `lm_robust` from *estimatr*

- reference to DeclareDesign framework
- https://declaredesign.org/r/estimatr/articles/getting-started.html#lm_robust

```{r}
library(estimatr)

m3 <- lm_robust(ln_wage ~ age + tenure + union + tenure:union + idcode,
                clusters = idcode,
                data = nlswork)
summary(m3)
```
- why NAs in F-statistic?
`- directly set fixed effects variable: nice since it also provides cleaner output and acc. to manual should be faster

```{r}
m3fe <- lm_robust(ln_wage ~ age + tenure + union + tenure:union,
                  clusters = idcode,
                  fixed_effects = ~idcode,
                  data = nlswork)
summary(m3fe)
```
- results the same as for `m3`
- 

- another good thing: no need to drag along vcov matrix
- FE model doesn't work with `margins`:

```{r}
#margins(m3fe, variables = 'tenure', at = list(union = 0:1)) %>% summary()      # doesn't work
```
- this works:

```{r, warning=FALSE}
margins(m3, variables = 'tenure', at = list(union = 0:1)) %>% summary()
```
- produces warnings "In sqrt(var_fit) : NaNs produced"
- SEs smaller than with sandwich
- uses different SE calculation by default (see math. notes https://declaredesign.org/r/estimatr/articles/mathematical-notes.html#lm_robust-notes)
- to achieve same result as with sandwich/lmtest use "stata" SEs:

```{r}
m3stata <- lm_robust(ln_wage ~ age + tenure + union + tenure:union + idcode,
                     clusters = idcode,
                     se_type = 'stata',
                     data = nlswork)
summary(m3stata)
```
```{r, warning=FALSE}
margins(m3stata, variables = 'tenure', at = list(union = 0:1)) %>% summary()
```

## Performance comparison

- performance can be crucial for large datasets
- baseline: lm and sandwich
- `lm.cluster` should have similar performance to lm and sandwich
- three options for `lm_robust`
- for fair comparison: don't calc. CIs (which `lm_robust` by default does)

```{r}
library(microbenchmark)

bench_lm_sandwich <- function() {
  m <- lm(ln_wage ~ age + tenure + union + tenure:union + idcode, data = nlswork)
  vcovmat <- vcovCL(m, cluster = ~idcode)
}

bench_lm_cluster <- function() {
  lm.cluster(ln_wage ~ age + tenure + union + tenure:union + idcode,
                 cluster = 'idcode',
                 data = nlswork)
}

bench_lm_robust1 <- function() {
  lm_robust(ln_wage ~ age + tenure + union + tenure:union + idcode,
            clusters = idcode,
            ci = FALSE,
            data = nlswork)
}

bench_lm_robust2 <- function() {
  lm_robust(ln_wage ~ age + tenure + union + tenure:union + idcode,
            clusters = idcode,
            se_type = 'stata',
            ci = FALSE,
            data = nlswork)
}

bench_lm_robust3 <- function() {
  lm_robust(ln_wage ~ age + tenure + union + tenure:union,
            clusters = idcode,
            fixed_effects = idcode,
            se_type = 'stata',
            ci = FALSE,
            data = nlswork)
}

microbenchmark(bench_lm_sandwich(),
               bench_lm_cluster(),
               bench_lm_robust1(),
               bench_lm_robust2(),
               bench_lm_robust3(),
               times = 30)
```

- specifying `fixed_effects` should speed things up, but in this case it doesn't

- now with CIs:

```{r}
bench_lm_sandwich_ci <- function() {
  m <- lm(ln_wage ~ age + tenure + union + tenure:union + idcode, data = nlswork)
  vcovmat <- vcovCL(m, cluster = ~idcode)
  ci <- coefci(m, cov = vcovmat)
}

bench_lm_robust1_ci <- function() {
  lm_robust(ln_wage ~ age + tenure + union + tenure:union + idcode,
            clusters = idcode,
            ci = TRUE,
            data = nlswork)
}

bench_lm_robust2_ci <- function() {
  lm_robust(ln_wage ~ age + tenure + union + tenure:union + idcode,
            clusters = idcode,
            se_type = 'stata',
            ci = TRUE,
            data = nlswork)
}

microbenchmark(bench_lm_sandwich_ci(),
               bench_lm_robust1_ci(),
               bench_lm_robust2_ci(),
               times = 30)
```

- Stata-type errors seem to be much faster to calculate

## Comparision of results with Stata

TODO

## Conclusion

- sandwich+lmtest / lm.cluster: super careful not to forget to pass the vcov mat!
- better with estimatr: vcov information inside model -> is used correctly by other functions like margins
- nested clusters: not possible with estimatr (?)

